---
title: "BUAN 573: Week 7 Assignment"
author: "Wenkkatessh"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r, echo=FALSE, warning=FALSE}
# Some housekeeping code
rm(list=ls()) # # remove all the variables in the environment
RNGkind(sample.kind="Rounding") # set the random number generator kind

#Color Format (any color)
colr = function(x,color){
  outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}

#Color Format (green color)
green = function(x){
  outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{green}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='green'>",x,"</font>",sep="")
  else
    x
}
```



# Step 1: Exploratory Data Analysis (EDA)
## In step 1, we are going to perform the initial investigations on our datasets so as to discover patterns,to spot anomalies our dataset might have and to check assumptions with the help of summary statistics and graphical representations(visualizations).It is alwasy a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand,before getting them dirty with it.

## 1. Importing the Fundraising.csv dataset and displaying its structure
```{r, echo=FALSE, warning=FALSE}
#load required packages
library(readr) #load csv

#First dataset
fund.df <- read.csv("Fundraising.csv")

future.fund.df <- read.csv("Fundraising.csv")

#Exclude the Row Id, Row Id., TARGET_D variable from the dataset
library(dplyr)
new.fund.df <- fund.df[ , -c(24)]

#get the structure of our dataset
str(new.fund.df)
```


## 1. Importing the FutureFundraising.csv dataset and displaying its structure
```{r, echo=FALSE, warning=FALSE}
#load required packages
library(readr) #load csv

#Second dataset
future.fund.df <- read.csv("Fundraising.csv")

#get the structure of our dataset
str(future.fund.df)
```

## 2.Below is the dimension of the Fundraising.csv dataset
```{r, echo=FALSE, warning=FALSE}

#get the dataset dimesnion
dim(new.fund.df)
```


## 3.Displaying the Descriptive Statistics of our dataset
```{r, echo=FALSE, warning=FALSE}

#load required packages
library(Hmisc)

#get the dataset dimension
describe(new.fund.df)
```

## 2.Clean the dataset
### We clean the dataset by removing the TARGET_D which will not be used in our case
### Covert categorical variables into factor data type
```{r, echo=FALSE, warning=FALSE}

library(dplyr)
#Remove the TARGET_D using its index number
clean.fund.df <- fund.df[ , -c(24)]
```


## Visualizing the dataset
### This is achieved by exploring some of the important variables
### Barplot Ranking of Homes Value (HV) by Wealth
```{r, echo=FALSE, warning=FALSE}

library(ggplot2)

#Ranking of Homes Value (HV) by Wealth
ggplot(data=clean.fund.df,aes(x=WEALTH,y=HV)) + 
  geom_bar(stat ='identity',aes(fill=HV))+
  coord_flip() + 
  theme_grey() + 
  scale_fill_gradient(name="Homes Value Level")+
  labs(title = 'Ranking of Homes Value (HV) by Wealth',
       y='HomeValue(HV)',x='Wealth')+ 
  geom_hline(yintercept = mean(clean.fund.df$HV),size = 1, color = 'blue')
```
### Boxplot

```{r, echo=FALSE, warning=FALSE}
clean.fund.df$gender.dummy <- factor(clean.fund.df$gender.dummy, 
                 levels=c(0,1), 
                 labels=c("male","female"))

ggplot(data = clean.fund.df, aes(x=gender.dummy,y=TARGET_B, fill=gender.dummy)) + 
  geom_boxplot()+
  scale_fill_brewer(palette="Green") + 
  geom_jitter(shape=16, position=position_jitter(0.2))+
  labs(title = 'Did males donate more than females?',
       y='Donation',x='Gender')

```
### Barplot Ranking Gender by frequency

```{r, echo=FALSE, warning=FALSE}

# Plotting a bar plot

ggplot(data = clean.fund.df) +
  geom_bar(mapping = aes(x = gender.dummy))
```




## Correlation 
```{r, echo=FALSE, warning=FALSE}

df <- clean.fund.df[,c(1,3,4,6,7,9,10)] #select relevant columns

res <- cor(clean.fund.df[,-c(1,2,7,10,24)]) #  look at all columns except the first,sec, factor variables and last variable

```

## Correlation Plot
```{r, echo=FALSE, warning=FALSE}

df <- clean.fund.df[,c(1,3,4,6,7,9,10)] #select relevant columns

res <- cor(clean.fund.df[,-c(1,2,7,10,24)]) # -1 look at all columns except the first,sec, and last 

# loading corrplot
library(corrplot)
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```


# Step 2: Methodology(Data Mining Techniques Used)

## In our case, we have been instructed to use three models in implementation of our predictive models. The 3 selected models were: Logistic regression, Classification tree and Neural Networks. Among these 3 models, we are to find one as the best model and perform testing using it.
## Logistic Regression  -  we have used this model since it will help in estimating the probability of an individulas to donate or not to donate based on our given fundraising dataset of independent variables. The dependent variable in our case is TARGET_B and is bounded between 0 and 1.

## Classification tree  -  A classification tree identifies what combination of our dataset factors  best differentiates between individuals(donors/not donors) based on our categorical variable of interest which is (TARGET_B)

## Neural Networks  - is a technique applied in our dataset in order to find hidden patterns in our fundraising dataset.


# Step 3: Use different models
## STEP 1:Partitioning
```{r}

# use set.seed() to get the same partitions when re-running the R code.
set.seed(12345)
## partitioning into training (60%) and validation (40%)
# randomly sample 60% of the row IDs for training; the remaining 40% serve as
# validation
train.rows <- sample(rownames(new.fund.df), dim(new.fund.df)[1]*0.6)
valid.rows <- sample(setdiff(rownames(new.fund.df), train.rows),dim(new.fund.df)[1]*0.2)
# assign the remaining 20% row IDs serve as test
test.rows <- setdiff(rownames(new.fund.df), union(train.rows, valid.rows))
# create the 3 data frames by collecting all columns from the appropriate rows
train.data <- new.fund.df[train.rows, ]
valid.data <- new.fund.df[valid.rows, ]
test.data <- new.fund.df[test.rows, ]
# 
#print the train data
head(train.data, n=5)
#print the validation data
head(valid.data, n=5)
#print the test data
head(test.data, n=5)
```

## STEP 2:Model Building

### Classification under asymmetric response and cost
### Weighted sampling allow us to reconfigure the sample as if it was a simple random draw of the whole dataset, and hence yield accurate dataset estimates for the main parameters of interest than when compared to the random sampling

### Classification tools, Net Profit and Lift curves for each model

### Logistic regression
```{r, echo=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)


fund.df <- read.csv("Fundraising.csv")

#Exclude the Row Id, Row Id., TARGET_D variable from the dataset
library(dplyr)
new.fund.df <- fund.df[ , -c(1, 2, 24)]

# partition
set.seed(12345)
train.index <- sample(c(1:dim(new.fund.df)[1]), dim(new.fund.df)[1]*0.6)
train.data <- new.fund.df[train.index, ]
valid.data <- new.fund.df[-train.index, ]

# run logistic regression
# use glm() (general linear model) with family = "binomial" to fit a logistic
# regression.
logit.reg <- glm(TARGET_B ~ ., data = train.data, family = "binomial")
options(scipen=999)
summary(logit.reg)

# use predict() with type = "response" to compute predicted probabilities.
logit.reg.pred <- predict(logit.reg, valid.data, type = "response")
# first 5 actual and predicted records
data.frame(actual = valid.data$TARGET_B[1:5], predicted = logit.reg.pred[1:5])

table_mat <- table(valid.data$TARGET_B, logit.reg.pred)

accuracy <- sum(diag(table_mat)) / sum(table_mat)

print(paste('Accuracy of Logistic Regression is', accuracy))

```


```{r, echo=FALSE, warning=FALSE}

library(gains)
gain <- gains(valid.data$TARGET_B, logit.reg.pred, groups=length(logit.reg.pred))
# plot lift chart
plot(c(0,gain$cume.pct.of.total*sum(valid.data$TARGET_B))~c(0,gain$cume.obs),
xlab="Number mailed", ylab="Net Profit", main="Logistic Regression Net Profit Lift Curve", type="l")
lines(c(0,sum(valid.data$TARGET_B))~c(0, dim(valid.data)[1]), lty=2)

```

### Classification Trees
```{r, echo=FALSE, warning=FALSE}

library(rpart)
library(rpart.plot)


fund.df <- read.csv("Fundraising.csv")

#Exclude the Row Id, Row Id., TARGET_D variable from the dataset
library(dplyr)
new.fund.df <- fund.df[ , -c(1, 2, 24)] 

# partition
set.seed(12345)
train.index <- sample(c(1:dim(new.fund.df)[1]), dim(new.fund.df)[1]*0.6)
train.data <- new.fund.df[train.index, ]
valid.data <- new.fund.df[-train.index, ]

# classification tree
classification.tree <- rpart(TARGET_B ~ ., data = new.fund.df, method = "class")
options(scipen=999)
# plot tree
prp(classification.tree, type = 1, extra = 1, under = TRUE, split.font = 1, varlen = -10)
```

```{r, echo=FALSE, warning=FALSE}

# use predict() with type = "response" to compute predicted probabilities.
classification.tree.pred <- predict(classification.tree, valid.data, type = "class")
# first 5 actual and predicted records
data.frame(actual = valid.data$TARGET_B[1:5], predicted = classification.tree.pred[1:5])


table_mat <- table(valid.data$TARGET_B, classification.tree.pred)

accuracy <- sum(diag(table_mat)) / sum(table_mat)

print(paste('Accuracy of Classification tree is', accuracy))

```

### Neural Networks model
```{r, echo=FALSE, warning=FALSE}
# install.packages("neuralnet")
#load required library
library(neuralnet)
library(nnet)
library(caret)

fund.df <- read.csv("Fundraising.csv")

myneuralnet <- neuralnet(TARGET_B ~ INCOME + WEALTH + HV, data = clean.fund.df, linear.output = F, hidden = 10)

# use predict() with type = "response" to compute predicted probabilities.
myneuralnet.pred <- predict(myneuralnet, valid.data, type = "class")

table_mat <- table(valid.data$TARGET_B, myneuralnet.pred)

accuracy <- sum(diag(table_mat)) / sum(table_mat)

print(paste('Accuracy of Neural Networks is', accuracy))

```




```{r, echo=FALSE, warning=FALSE}

# use predict() with type = "response" to compute predicted probabilities.
myneuralnet.pred <- predict(myneuralnet, valid.data, type = "class")
# first 5 actual and predicted records
data.frame(actual = valid.data$TARGET_B[1:5], predicted = myneuralnet.pred[1:5])


library(gains)
gain <- gains(valid.data$TARGET_B, myneuralnet.pred, groups=length(myneuralnet.pred))
# plot lift chart
plot(c(0,gain$cume.pct.of.total*sum(valid.data$TARGET_B))~c(0,gain$cume.obs),
xlab="Number mailed", ylab="Net Profit", main="Neural Network Net Profit Lift Curve", type="l")
lines(c(0,sum(valid.data$TARGET_B))~c(0, dim(valid.data)[1]), lty=2)

```

### Selected model  -  Classification tree
### Classification tree is the best model because of its high accuracy on our dataset when compared with logistic and Neural Networks predictive models.

## STEP 3: Testing
```{r, echo=FALSE, warning=FALSE}

future.fund.df <- read.csv("Fundraising.csv")

testing_mat <- future.fund.df[order(future.fund.df$Row.Id),]
test <- data.frame(testing_mat$Row.Id.,testing_mat$TARGET_B)
test.results <- test[order(-testing_mat$Row.Id.),]

head(test.results, n=5)

```


### Number (6) - In this step, we are supposed to find the probability of people donating using the Futurefundraising dataset and thereafter arrange the results in decsinding order and make conlcusions frtom what we are seeing. From the testing results, when you keenly analyse the results from the display, its clear that most people were not willing to donate. Therefore I would'nt  go on with the mailing campaign since most people were not willing to donate






